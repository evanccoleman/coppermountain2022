\documentclass{article}

\usepackage{latexsym}
\usepackage{graphicx,caption}
\usepackage{mathptmx}
\usepackage{tikz}

\usepackage[margin=1in]{geometry}

%
%****************************************************************************
%  AUTHOR: You may want to use some of these packages. (Optional)
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}
%****************************************************************************


% Custom commands
\newcommand{\mcomment}[2]{\textcolor{red}{#2}\marginpar[\color{red}\textbf{\hskip 60pt {#1}} $\Leftarrow$]{{\color{red}\textbf{#1 Note}}}}
\newcommand{\annotate}[1]{{\color{blue}#1}}
\newcommand{\annotatenew}[1]{{\color{red}#1}}

% Added packages
\usepackage[linesnumbered,lined,noline,ruled,noend]{algorithm2e}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{epsfig,graphicx}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{booktabs}
\usepackage{paralist}
\usepackage[pdftex,colorlinks=true,urlcolor=blue,citecolor=black,anchorcolor=black,linkcolor=black]{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{fancyhdr}
\usepackage{subcaption}

%%% Math environments
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{example}
\newtheorem{example}{Example}

\theoremstyle{example}
\newtheorem{remark}{Remark}

\theoremstyle{example}
\newtheorem{assumption}{Assumption}

% Neat trick for (scaled) ceiling and floor functions
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\title{Dynamic Non-Uniform Randomization in Asynchronous Linear Solvers}
\author{Evan Coleman \and Masha Sosonkina}
\date{}



%#########################################################
%*
%*  The Document.
%*
\begin{document}
	
\maketitle

\begin{abstract}
	Simple document to list new algorithms along with existing algorithms in the literature along with proves of convergence.
\end{abstract}
% Keywords
\noindent
{\em {\bf Keywords:} asynchronous iteration, linear solver, randomized linear algebra}


%------------------------------------------------------------------------------
% Introduction
%
\section{Background}
\label{sec:background}


%------------------------------------------------------------------------------
% Stationary/Relaxation Methods
%
\subsection{Asynchronous Iterative Methods}
\label{sect:asynchronous-iterative-methods}

In asynchronous computation, each part of the problem is updated such that no information from other parts is needed while each individual computation is performed. This allows each processor to act independently. The model that is shown here to provide a basis for asynchronous computation comes mainly from \cite{frommer2000asynchronous}. To start, consider a fixed point iteration with the function, $G: D \rightarrow D$. Given a finite number of processors ${P_1, P_2, \ldots, P_p}$  each assigned to a block {\cal B} of components ${B_1, B_2, \ldots, B_m}$, the computational model can be stated in \cref{algo:computational-model-1}.

%The algorithm2e code for the Asynchronous Iterative Computational Model (Frommer & Szyld 2000)
\begin{algorithm}
	\DontPrintSemicolon
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Read $x$ from common memory \;
			Compute $x_j^{i+1} = G_j(x)$ for all  $j \in {\cal B}_{l}$ \;
			Update $x_j$ in common memory with $x_j^{i+1}$ for all
			$j \in {\cal B}_{l}$ \;
		}
	}
	\caption{General Computational Model}
	\label{algo:computational-model-1}
\end{algorithm}

If each processors ($P_l$) waits for the other processors to finish each update, then the model describes a parallel synchronous form of computation. If no order is established for the processors, then the computation is asynchronous. At the end of an update by processor $p$, the components associated with the block $B_p$ will be updated. This results in a vector, $x = (x_1^{s_1(k)}, x_2^{s_2(k)}, \ldots, x_m^{s_m(k)})$ where $s_l(k)$ indicates how many times component $l$ has been updated, and $k$ is a global iteration counter. A set of indices $I^k$ contains the components that were updated on the $k^{th}$ iteration. Given these definitions, the three following conditions provide a framework for asynchronous computation:

\begin{definition}
	\label{def:asynchronous-model}
	If the following three conditions hold:
	\begin{enumerate}
		\item	$s_i(k) \leq k - 1$, {\em i.e., only components that have finished computing are used in the current approximation.}
		\item	$\lim_{k\rightarrow \infty} s_i(k) = \infty$, {\em i.e., the newest updates for each component are used.}
		\item	$|{k \in \mathbb{N}: i \in I^k}| = \infty$, {\em i.e., all components will continue to be updated.}
	\end{enumerate}
	Then given an initial $x^0 \in D$, the iterative update process defined by,
	\[ x_i^{(k)} =
	\begin{cases}
	x_i^{(k-1)} 		& 	i \notin I^k \\
	G_i(x^{(k)}) 		&	i \in    I^k
	\end{cases}
	\]
	where the individual functions $G_i(\vec{x})$ use the latest updates available, is called an asynchronous iteration.
\end{definition}

This basic computational model (i.e. the combination of \cref{algo:computational-model-1} and \cref{def:asynchronous-model} together) allows for many different results on fine-grained iterative methods.


%------------------------------------------------------------------------------
% Randomized Parallel Southwell Algorithm
%
\subsection{Randomized Linear Solvers}
\label{sect:randomized-linear-solvers}

Relaxation methods have been the focus of many studies related to asynchronous iterations starting with \cite{chazan1969chaotic} and \cite{baudet1978asynchronous}. They are typically used to solve linear systems of the form $Ax = b$ and can be written as fixed point iterations that can be expressed as
\begin{equation}
\label{eq:fixed-point-iteration-matrix}
x^{k+1} = C x^k + d\; ,
\end{equation}
\noindent
where $C$ is the $n\times n$ iteration matrix, $x$ is an $n$-dimensional vector that represents the solution, and $d$ is another $n$-dimensional vector that can be used to help define the particular problem at hand. The Jacobi method is a relaxation method that can be used in an asynchronous manner and the update for a given component $x_i$ can be expressed as
\begin{equation}
\label{eq:element-wise-jacobi}
x_i = \frac{-1}{a_{ii}}\left[\sum_{j\neq i} a_{ij}x_j - b_i \right].
\end{equation}
\noindent
This iteration can give successive updates to the $x_i$ component in the solution vector. In synchronous computing environments, each update to an element of the solution vector, $x_i$, is computed sequentially using the same data for the other components of the solution vector (i.e., the values for $x_j$ in \cref{eq:element-wise-jacobi}). Conversely, in an asynchronous computing environment, each update to an element of the solution vector occurs when the computing element responsible for updating that component is ready to write the update to memory and the other components used are simply the latest ones available to the computing element. Expressing \cref{eq:element-wise-jacobi} in a block form similar to \cref{eq:fixed-point-iteration-matrix} gives an iteration matrix of $C = -D^{-1}(L+U)$ where $D$ is the diagonal portion of $A$, and $L$ and $U$ are the strictly lower and upper triangular portions of $A$ respectively. Convergence of asynchronous fixed point methods of the form presented in \cref{eq:fixed-point-iteration-matrix} is determined by the spectral radius of the iteration matrix, $C$. 

\begin{theorem}
	\label{th:fixed-point-convergence}
	For a fixed point iteration of the form given in \cref{eq:fixed-point-iteration-matrix} that adheres to the	asynchronous computational model provided by \cref{algo:computational-model-1} and \cref{def:asynchronous-model}, if the spectral radius of $C$, $\rho(|C|)$, ls less than one, then the iterative method will converge to the fixed point solution.
\end{theorem}

Asymptotic results such as this, i.e. that guarantee eventual convergence but offer no guarantee as to the rate of that convergence, exist for many variants of the iteration described above (see \cite{frommer2000asynchronous} for a summary). The use of randomization in asynchronous linear solvers allows for the possibility of statements concerning the rate of convergence to be made. Leventhal and Lewis introduced a randomized Gauss-Seidel method \cite{leventhal2010randomized} building off of the randomized Kaczmarz algorithm proposed by Strohmer and Vershynin \cite{strohmer2009randomized}. The analysis was generalized by Griebel and Oswald who also added a new parameter that allows for both over and under relaxation \cite{griebel2012greedy}. The work that much of the analysis in the paper is based off of is due Avron, Druinsky, and Gupta who build upon the analysis and explicitly analyze the case of asynchronous computation \cite{avron2015revisiting}.  

Each of the methods select the vector component to update (see \cref{eq:element-wise-jacobi}) from a random distribution instead of either sequentially looping through the available components or by tying the updates for a single component to a particular processor. In a traditional parallelization of either a synchronous or asynchronous linear solver, processor $j$ is responsible for updating component $j$; the asynchronous variant allows processor $j$ to continue to compute relaxations for the component assigned to it regardless of the state of the other processors. The use of randomization in the selection of which component to update allows for the possibility of any processor updating any component.

In a randomized asynchronous linear solver, when a processor finishes computing an update to a component, it writes the update to shared memory and then randomly draws the next component to update from the list of all available components. In practice, a processor will often select a contiguous block of components to update as opposed to an individual row. The updating of components inside of the block can be done in a variety of different ways, but will often be done sequentially using a traditional Gauss-Seidel algorithm. Throughout this analysis, a {\em consistent read} (using the terminology from \cite{avron2015revisiting}) is assumed; whereby no processor is partway through writing a block to memory while another processor is reading it. This could cause an update to be performed using a vector, $x$, that never existed. Though the asynchronous nature of the computation allows the possibility of an inconsistent read from memory without using atomic operations to enforce safety, Avron et al point out that such a read is very uncommon in practice \cite{avron2015revisiting}.

In the randomized asynchronous linear solvers proposed to date, this random selection is always done using either uniform random number generation, or with a probability proportional to a row norm of the matrix $A$. Leventhal and Lewis cite Fourier analysis \cite{leventhal2010randomized} as an application area that can benefit from this type of weighting; however, there is no reason to expect improved behavior for an arbitrary problem. A major point of exploration here is to investigate the feasibility of using non-uniform distributions in the selection of which component to update.


\section{Algorithms}
\label{sect:algorithms}

The Southwell algorithm \cite{southwell1946relaxation} works similar to Jacobi by relaxing a single equation at a time, but chooses the equation with the largest local residual. This difference allows the Southwell algorithm to often converge in fewer iterations than Jacobi, but raises the expense of computing an update since the local residuals need to be stored and ranked at each iteration. For example, after a given iteration, the Southwell algorithm will choose the component that contributes the most to the global residual to update; in order to do this, the algorithm must rank the residuals from largest to smallest.

\begin{enumerate}
	\item pick a random $j \in \{1, 2, \ldots, n\}$ under the assumption there are $n$ equations in the matrix
	\item read the corresponding entries of $A, x, b$
	\item perform the relaxation for equation $x_j$
	\item update the data for $x_j$
\end{enumerate}

Using the insight from the Southwell algorithm and the success found in the parallel Southwell implementations~\cite{wolfson2016reducing,wolfson2017distributed}, the idea behind the randomized linear solvers considered here is for each processor to select the next component it is responsible for updating randomly, using a distribution that more heavily weights selection of components that contribute more to the global residual.  Pseudo-code for a randomized variant is provided by \cref{algo:randomized-computational-model}, which is the same as the one for the randomized asynchronous Jacobi presented in \cite{avron2015revisiting}, where a  uniform distribution over $\{1, 2, \ldots, n\}$ is studied. The key difference of the present work is that here non-uniform distributions  in~\cref{alg:pickrandom} of~\cref{algo:randomized-computational-model} are investigated. 

\begin{algorithm}[ht!]
	\DontPrintSemicolon
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Pick $j \in \{1, 2, \ldots, n\}$ using a given probability distribution \; \label{alg:pickrandom}
			Read the corresponding entries of $A, x, b$ \label{step:read}\;
			Perform the relaxation for equation $x_j$ \label{step:relax}\;
			Update the data for $x_j$ \;
		}
	}
	\caption{Generic Randomized Linear Solver}
	\label{algo:randomized-computational-model}
\end{algorithm}

The goal behind many of the proposed modifications to the Generic Randomized Linear Solver presented in \cref{algo:randomized-computational-model} is that relaxing the components with a more significant contribution to the global residual may reduce the total number of iterations required. Motivation for this comes from a myriad of different studies, see for instance the paper by Nutini et al that shows that for some cases (Gauss-)Southwell selection can converge faster than uniform random selection for coordinate descent \cite{nutini2015coordinate}. In general, the improvement in convergence will have to be shown to be significant enough to offset the extra computational and communication cost associated with storing and ranking all of the local residuals. 

In an effort to simulate the effect of the Southwell algorithm using randomized asynchronous solvers,  the {\em local residuals} associated with each equation (or block of equations) are ranked and sorted, and the selection of the next equation (i.e., component) to update is performed using a non-uniform distribution that forces the random selection to pick components with larger local residuals more frequently. The Southwell algorithm \cite{southwell1946relaxation} works similar to Jacobi by relaxing a single equation at a time, but chooses the equation with the largest local contribution to the residual. For a given row (or block of rows) $i$, this local contribution is defined to be

\begin{equation}
r_i^{(k)} = b_i - Ax_i^{(k)}
\end{equation}

\noindent
at iteration $k$. This difference allows the Southwell algorithm to often converge in fewer iterations than Jacobi, but raises the expense of computing an update since the local residuals need to be stored and ranked at each iteration. For example, after a given iteration, the Southwell algorithm will choose the component that contributes the most to the global residual to update; in order to do this, the algorithm must rank the residuals from largest to smallest. Since ranking and sorting local residuals can be expensive, the periodicity with which this is done contributes to the overall efficiency of the algorithm. Previously in \cite{jensen2018using}, the authors have studied a balance between computational effort spent performing relaxations compared with other algorithmic operations and communications in asynchronous methods.

\subsection{Existing Algorithms}
\label{sect:existing-algorithms}

Before commencing analysis of the new variants, a brief review of existing results for similar algorithms is provided. The main goal of this type of analysis is first to verify convergence of the algorithm, and then provide convergence rate analysis when possible. The first result is due to Leventhal and Lewis \cite{leventhal2010randomized}. In this variant of the algorithm, the component $j$ is chosen with probability,
	\begin{equation}
		\mathbb{P}\{j = k\} = \frac{a_{kk}}{tr(A)}.
	\end{equation}
\noindent
Given this, the following result from \cite{leventhal2010randomized} establishes a bound on convergence rate. Note that this algorithm is analyzed from a deterministic point of view in \cite{leventhal2010randomized} that neglects the effects of asynchronous parallelism.

\begin{theorem}[Leventhal, Lewis]
	If $A$ is symmetric and positive-definite, then the variant of \cref{algo:randomized-computational-model} due to Leventhal and Lewis described above satisfies the property,
		\begin{equation}
			\mathbb{E}[\Vert x^{(j)} - x^* \Vert_A^2] \leq \left(1 - \frac{1}{\Vert A^{-1} \Vert_2 tr(A)}\right) \Vert x^{(j-1)} - x^* \Vert_A^2
		\end{equation}
	which shows that the iteration converges linearly in the $A$-norm.
\end{theorem}

The next result, due to Griebel and Oswald \cite{griebel2012greedy}, adds an over/under relaxation parameter, $\beta$, to the algorithm. Note that \cref{step:relax} of \cref{algo:randomized-computational-model} can be written,

\begin{equation}
	x_j^{(i+1)} = x_j^{(i)} + \frac{\beta}{a_{ii}} r_j^{(i)}
\end{equation}
\noindent
That is, the relaxations performed in \cref{step:relax} of \cref{algo:randomized-computational-model} are weighted by a step size parameter before they are applied. This new algorithm serves as a starting point for the analysis continued below and is expressed explicitly in \cref{algo:randomized-computational-model2}.

\begin{algorithm}[ht!]
	\DontPrintSemicolon
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Pick $j \in \{1, 2, \ldots, n\}$ using a given probability distribution \; \label{alg:pickrandom2}
			Read the corresponding entries of $A, x, b$ \label{step:read2}\;
			Compute $x^{(i)}_j = x^{(i-1)}_j + \frac{\beta}{a_{ii}} r^{(i)}_j$ \;
			Update the data for $x^{(i)}_j$ \;
		}
	}
	\caption{Updated Randomized Linear Solver}
	\label{algo:randomized-computational-model2}
\end{algorithm}

In this case, linear convergence is maintained, but the bound changes slightly. The corresponding result is due to Griebel and Oswald \cite{griebel2012greedy} and is provided below.

\begin{theorem}[Griebel, Oswald]
	If $A$ is symmetric and positive-definite, then the variant of \cref{algo:randomized-computational-model} due to Griebel and Oswald described above satisfies the property,
		\begin{equation}
			\mathbb{E}[\Vert x^{(j)} - x^* \Vert_A^2] \leq \left(1 - \frac{\beta (2 - \beta)\lambda_{min}}{tr(A)}\right)^m \Vert x^{(0)} - x^* \Vert_A^2
		\end{equation}
	which shows that the iteration converges linearly in the $A$-norm.
\end{theorem}
	

When the selection in \cref{alg:pickrandom} of \cref{algo:randomized-computational-model2} is done using a combination of: a uniform distribution, an asynchronous mode of computation (with a consistent read model), $A$ taken to be symmetric and positive-definite, and there is a bound, denoted $\tau$, on how out-of-date information for any component can be, then the following result gives a bound on convergence rate.

\begin{theorem}[Avron, Druinsky, Gupta]
	For an arbitrary starting vector, $x^{(0)}$, if $\rho = \frac{1}{n} \Vert A \Vert_\infty$, then if $2\rho \tau < 1$ the following hold:
		\begin{enumerate}
			\item For every $m \geq \frac{log(1/2)}{log(1-\lambda_{max}/n)} \approx \frac{0.693n}{\lambda_{max}}$,
					\begin{equation}
						\mathbb{E}[\Vert x^{(m)} - x^* \Vert_A^2] \leq \left( 1 - \frac{\nu_\tau}{2\kappa(A)}\right) \cdot \mathbb{E}[\Vert x^{(0)} - x^* \Vert_A^2]
					\end{equation}
				where
					\begin{equation}
						\nu_\tau = 1 - 2 \rho \tau
					\end{equation}
			\item Let $T_0 = \ceil*{\frac{log(1/2)}{log(1 - \lambda_{max}/n)}}$ and $T = T_0 + \tau$. For every $m \geq rT (r = 1, 2, \ldots)$,
					\begin{equation}
						\mathbb{E}[\Vert x^{(m)} - x^* \Vert_A^2] \leq \left( 1 - \frac{\nu_\tau}{2\kappa(A)}\right) \left( 1 - \frac{\nu_\tau(1 - \lambda_{max}/n)^\tau}{2\kappa(A)} + \chi \right) \mathbb{E}[\Vert x^{(0)} - x^* \Vert_A^2]
					\end{equation}
				where 
					\begin{equation}
						\chi = \frac{\rho \tau^2 \lambda_{max}(1 - \lambda_{max}/n)^{-2\tau}}{n}
					\end{equation}			
			\end{enumerate}
\end{theorem}

\subsection{New Algorithms}
\label{sect:new-algorithms}

The analysis for the new variants of the algorithm picks up from this point. For the following discussion, assume that $A$ is symmetric and positive-definite. All of these variants are built around exploiting a Southwell like selection of the next component $j$. As a foundation for the randomized variants, a Southwell-Schwarz method due to Griebel and Oswald is presented first in \cref{algo:southwell-schwarz-linear-solver}.

\begin{algorithm}[ht!]
	\DontPrintSemicolon
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Pick an index, $j \in \{1, 2, \ldots, n\}$ such that $r_j^{(i)} \geq \alpha \max_k r_k^{(i)}$ \; \label{alg:picksouthwell}
			Read the corresponding entries of $A, x, b$ \label{step:read25}\;
			Compute $x^{(i)}_j = x^{(i-1)}_j + \frac{\beta}{a_{ii}} r^{(i)}_j$ \;
			Update the data for $x^{(i)}_j$ \;
		}
	}
	\caption{Southwell-Schwarz Linear Solver}
	\label{algo:southwell-schwarz-linear-solver}
\end{algorithm}

The procedure detailed in \cref{algo:southwell-schwarz-linear-solver} selects (non-randomly) an index that is close to being the largest contributor to the residual. This algorithm makes use of a weakness parameter, $\alpha$ with $0 \leq \alpha \leq 1$, whereby if $\alpha = 0$ any component $j$ can be selected, and if $\alpha = 1$ the original Southwell method (where only the single component with the largest contribution to the residual is eligible to be selected) is obtained. The method for choosing the index from among the allowed indices defined by the weakness parameter $\alpha$ is not specified explicitly in the original paper \cite{griebel2012greedy}. Corresponding to this algorithm is another result detailing linear convergence with no reliance on the expected outcome since the algorithm no longer relies upon random processes.

\begin{theorem}[Griebel, Oswald]
\label{th:southwell-schwarz}
	If $A$ is symmetric and positive-definite, then the variant of \cref{algo:randomized-computational-model} due to Griebel and Oswald described above satisfies the property,
	\begin{equation}
	\Vert x^{(j)} - x^* \Vert_A^2 \leq \left(1 - \frac{\alpha^2 \beta (2 - \beta)\lambda_{min}}{tr(A)}\right)^m \Vert x^{(0)} - x^* \Vert_A^2
	\end{equation}
	which shows that the iteration converges linearly in the $A$-norm.
\end{theorem}

Updating all the residuals every iteration in order to make this selection as defined likely introduces too much computational overhead to be of practical use. Periodically ranking the residuals and setting $\alpha$ in a problem specific manner creates a dynamically changing list of indices eligible for selection. All of these eligible indices correspond to components with larger local contribution to the global residual. The thrust of the new algorithms is to focus on techniques that achieve this dynamic focus on components that contribute more to the residual, but to do it in a different manner that no longer excludes a large set of indices and that more explicitly defines the method for selecting the next index to use from the prescribed list. To to do this, the randomized variants will be modified to use distributions that are weighted to more often select components with larger contribution to the residual. This is motivated by the case presented in \cref{example:sorted-residuals}.

\begin{example}
\label{example:sorted-residuals}
	Consider the two dimensional finite-difference discretization of the Laplacian, $-\Delta u = f$, with Dirichlet boundary conditions taken over a $10 \times 10$ grid. This results in 100 different components to update. The initial residuals for each component are shown in \cref{fig:initial-residuals-laplacian} both unsorted (left) and sorted from largest to smallest (right). From \cref{sfig:sorted-residuals}, it may be clearly seen that using a non-uniform distribution that favors specific parts of the ``slope'' is effectively possible, and prioritizing the updates of the components contributing the most to the global residual may be beneficial to convergence similar to the Southwell method.
	
	\begin{figure}[ht!]
		\centering
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\textwidth]{images/init_resids_10x10.pdf}
			\caption{Unsorted Residuals}
			\label{sfig:unsorted-residuals}
		\end{subfigure}
		\begin{subfigure}{0.45\textwidth}
			\includegraphics[width=\textwidth]{images/init_resids_10x10_sorted.pdf}
			\caption{Sorted Residuals}
			\label{sfig:sorted-residuals}
		\end{subfigure}
		\caption{Initial component residuals ($r_i / \max (r_i)$).}
		\label{fig:initial-residuals-laplacian}
	\end{figure}
\end{example}

The sorted residuals in \cref{sfig:sorted-residuals} indicate that a non-uniform distribution that targets components with larger contribution to the residual, but not necessarily restricted to the component with the single largest contribution, could be beneficial. For example, \cref{fig:distribution-residuals} shows an overlay of exponential distributions on the sorted, normalized component residuals at initialization, after each component has been relaxed 10 times, and after convergence has been achieved.

\begin{figure}[ht!]	
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{images/exp_dist_overlay_init.pdf}
		\caption{Initial residuals}
		\label{exp_dist_init}
	\end{subfigure}		
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{images/exp_dist_overlay_10iter.pdf}
		\caption{Residuals at 10 iterations}
		\label{exp_dist_10iter}
	\end{subfigure}		
	\begin{subfigure}{0.33\linewidth}
		\includegraphics[width=\textwidth]{images/exp_dist_overlay_final.pdf}
		\caption{Final residuals}
		\label{exp_dist_final}
	\end{subfigure}
	
	\caption{Sorted residuals ($r_i / \max (r_i)$) with exponential distributions for reference.}
	\label{fig:distribution-residuals}
\end{figure}

The a non-dynamic variant of the algorithm corresponding to using a weighted probability distribution (such as an exponential distribution) is given in \cref{algo:randomized-computational-model3}.

\begin{algorithm}[ht!]
	\DontPrintSemicolon
	Compute component residuals \;
	Rank and sort the component residuals \;
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Pick $j \in \{1, 2, \ldots, n\}$ using a weighted probability distribution \; \label{alg:pickrandom3}
			Read the corresponding entries of $A, x, b$ \label{step:read3}\;
			Compute $x^{(i)}_j = x^{(i-1)}_j + \frac{\beta}{a_{ii}} r^{(i)}_j$ \label{step:update} \;
			Update the data for $x^{(i)}_j$ \;
		}
	}
	\caption{Weighted Randomized Linear Solver}
	\label{algo:randomized-computational-model3}
\end{algorithm}

However, one of the points of emphasis for the algorithms developed in this paper was the focus on dynamically changing the component selection process as the iteration progresses. To this end, each time a component residual is calculated (c.f. \cref{step:update} of \cref{algo:randomized-computational-model3}) the ranked and sorted list of component residuals can be updated. This variation is shown in \cref{step:dynamic-update} of \cref{algo:randomized-computational-model4}.

\begin{algorithm}[ht!]
	\DontPrintSemicolon
	Compute component residuals \;
	Rank and sort the component residuals \;
	\For {each processing element $P_{l}$} {
		\For {$i = 1, 2, \ldots$ until convergence} {
			Pick $j \in \{1, 2, \ldots, n\}$ using a weighted probability distribution \; \label{alg:pickrandom4}
			Read the corresponding entries of $A, x, b$ \label{step:read4}\;
			Compute $x^{(i)}_j = x^{(i-1)}_j + \frac{\beta}{a_{ii}} r^{(i)}_j$ \;
			Update the data for $x^{(i)}_j$ \;
			Update and re-sort the list of component residuals \label{step:dynamic-update} \;
		}
	}
	\caption{Dynamic Weighted Randomized Linear Solver}
	\label{algo:randomized-computational-model4}
\end{algorithm}

Note that updating the list of component residuals with every component update is not computationally viable, but adding a periodicity to the updating and ranking of the component residuals can make this randomized asynchronous linear solver variant more viable.

For the case of the weighted distribution being selected to be an exponential distribution with parameter $\mu$, the probability density function (PDF) is given by,

\begin{equation}
	\label{eq:exponential-pdf}
		f(x; \mu) =
		 \begin{cases*}
					\mu e^{-\mu x}	& if  $x \geq 0$  \\
								  0 & if $x < 0$
		 \end{cases*}
\end{equation}

In the limiting sense, the distribution becomes the Southwell iteration at one extreme, where only the component with largest contribution to the residual is selected, and becomes a uniform distribution at the other extreme, where all components have equal chance of being selected. If the PDF is modified to become 0 for all values $x$ that correspond to components whose residual is not within $\alpha$ of the max residual, then the result by Griebel and Oswald given in \cref{th:southwell-schwarz} can be applied.

\bibliographystyle{plain}
\bibliography{./bibs/Asynchronous-Applications,./bibs/Asynchronous-FT,./bibs/Asynchronous-Theoretical,./bibs/Example-Problems,./bibs/Fault-Tolerance,./bibs/Fixed-Point,./bibs/General-HPC,./bibs/HPC-Math-Software,./bibs/Iterative-Methods,./bibs/Preconditioning,./bibs/Self-Citations,./bibs/Textbooks}






\end{document}
